{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data1=pd.read_csv('nlp_data1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "X = nlp_data1['lemmatized_token']\n",
    "y = nlp_data1['target']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall',recall)\n",
    "print('f1', f1)\n",
    "\n",
    "joblib.dump(model, 'random_forest.pkl')\n",
    "loaded_model = joblib.load('random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7774130006565988\n",
      "precision 0.7980769230769231\n",
      "recall 0.6394453004622496\n",
      "f1 0.7100085543199315\n",
      "cofusion matrix [[769 105]\n",
      " [234 415]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "conf_matrix = confusion_matrix (y_test, y_pred)\n",
    "\n",
    "print('accuracy', accuracy)\n",
    "print('precision', precision)\n",
    "print('recall',recall)\n",
    "print('f1', f1)\n",
    "print('cofusion matrix', conf_matrix)\n",
    "\n",
    "joblib.dump(model, 'random_forest.pkl')\n",
    "loaded_model = joblib.load('random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "Accuracy is the proportion of correctly classified instances among all instances.\n",
    "In this case, the accuracy is approximately 0.7774, or 77.74%. This means that around 77.74% of the instances in the test set were classified correctly by the model.\n",
    "\n",
    "#### Precision:\n",
    "Precision is the proportion of true positive predictions among all positive predictions made by the model.\n",
    "In this case, the precision is approximately 0.7981, or 79.81%. This means that when the model predicts a positive class, it is correct around 79.81% of the time.\n",
    "\n",
    "#### Recall:\n",
    "Recall, also known as sensitivity or true positive rate, is the proportion of true positive predictions among all actual positive instances in the data.\n",
    "In this case, the recall is approximately 0.6394, or 63.94%. This means that the model correctly identifies around 63.94% of all actual positive instances.\n",
    "\n",
    "#### F1 Score:\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "In this case, the F1 score is approximately 0.7100. It takes into account both precision and recall, providing a single score that balances the trade-off between false positives and false negatives.\n",
    "\n",
    "#### Confusion Matrix:\n",
    "The confusion matrix provides a more detailed breakdown of the model's predictions.\n",
    "It is a 2x2 matrix where each cell represents the counts of instances according to their actual and predicted classes.\n",
    "\n",
    "In this confusion matrix:\n",
    "\n",
    "The top-left cell (769) represents true negatives (TN), meaning instances that were correctly predicted as negative.\n",
    "The top-right cell (105) represents false positives (FP), meaning instances that were incorrectly predicted as positive.\n",
    "The bottom-left cell (234) represents false negatives (FN), meaning instances that were incorrectly predicted as negative.\n",
    "The bottom-right cell (415) represents true positives (TP), meaning instances that were correctly predicted as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
